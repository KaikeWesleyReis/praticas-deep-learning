{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54f4b366",
   "metadata": {},
   "source": [
    "# Problem Context\n",
    "Was proposed a simple ecommerce search engine using `TF-IDF` using product name and description as input with none pre-processing. Final **MAP@10 was 0.29**.\n",
    "\n",
    "# Proposed Approach\n",
    "\n",
    "Use BM-25 (default) as search engine, using all \"easy to access\" product information (name, description, class and category) as input with a simple pre-processing that checks for some pitfalls: ignore empty tokens, keep numbers, remove punctuation and etc. Final **MAP@10 was 0.38**.\n",
    "\n",
    "# Questions to Answer\n",
    "\n",
    "**1) The search engine in the notebook has a MAP@10 across all queries of 0.29. This is considered low. Please propose some updates to increase the score. For reference, large ecommerce websites have MAP@10 values between 0.6 â€” 0.8, although there is no expectation for your solution to be in that range. The strength of your ideas holds greater weight than the final MAP score of the solution.**\n",
    "\n",
    "Why I proposed this simple approach?\n",
    "- Easier and fast to implement and deploy\n",
    "- It's well known that BM-25 is an improvement compared to TF-IDF in RAG scenarios (BM-25 was developed basically by this idea).\n",
    "- For sparse methods as TF-IDF and BM-25, it's really important to have text processing: remove sequential spaces, convert all to lower, remove stopwords and etc.\n",
    "- Let's not get complex at the beggining: I could simple use the embeddings + cosine similarity to solve the case, but would take a lot of time to process the embeddings to product dataset and BM-25 is a good start.\n",
    "- I used the other two columns based on the simple principle: more data = better model. I did not used `product_features` given the necessity to understand it better.\n",
    "\n",
    "So far, this method achieved **0.38**, which is an improvement. \n",
    "\n",
    "If I had more time and resources, what would I do? (Have in mind that, each topic that I present it here would be compared with my base model (0.40) before I move to the next topic)\n",
    "- Explore the column `product_features` to understand more your meaning and how would I extract useful information from the product. After that, if I found it useful, I would add in the `search_text` (with previous pre-processing);\n",
    "- Improve text pre-processing by a carefully analysis over my products info: the idea would to have a deeper knowledge over it to, hopefully, convert into a better pre-processing.\n",
    "- Implement stemming and lemmatization during pre-processing phase;\n",
    "- Optimize BM-25 hyper-parameters: k1 - b - version;\n",
    "- Implement in parallel a dense approach: Convert my `search_text` to a dense vector (embedding) and use cosine similarity to define my Top K (I started to implement, using sentence_transformers with open source models from hugging face, but would take a lot of time to generate to products dataset);\n",
    "- Optimize Embedding Selection Model (I know, would take a lot of time, but I want to mention it)\n",
    "- Compare BM-25 (sparse) and Embeddings (dense) approach to define for: hybrid (using both through RRF Score) or keep only one (reduce complexity - Would be using if one is way better than the other);\n",
    "\n",
    "The PROs and CONs:\n",
    "\n",
    "- Sparse with BM-25:\n",
    "    PROS - Fast to train, Low Latency, easy to deploy, Good Start and most of the time presents a good performance.\n",
    "    CONS - Lack of semantic understandment (home != house != apartment), necessity to pre-processing\n",
    "- Dense with Embeddings:\n",
    "    PROS - Semantic understandment, lack of dependencies (only need to load the data, the model and apply cosine similarity)\n",
    "    CONS - Complex to deploy when you have millions of items to search (should apply a reduction search algorithm as Faiss), slow to prototype compared to sparse methods\n",
    "- Hybrid Approach: Add complexity to deployment\n",
    "\n",
    "**2) Currently, partial matches are treated as irrelevants, which penalizes the model too strictly. Can you implement another function that leverages the partial match count to provide a fairer assessment of performance? Please provide a justification for why you chose this function and the tradeoffs. If you choose to implement additional evaluation metrics, please provide a justification for using them along with tradeoffs.**\n",
    "\n",
    "I proposed to weight **MAP@K** metric, considering the weight of 1 to exact case and weight of 0.5 to partial cases. I validated this metric later by checking if the results for only exact cases using your implementation matches my custom weighted implementation.\n",
    "\n",
    "I consider that would be an easier approach given my time constrains.\n",
    "\n",
    "**3) For this prompt you can choose one of two options, but you DO NOT need to do both. We value the ability to improve model performance and refactor code equally, so please choose based on what you feel most comfortable doing: (A) Please implement at least one change you suggest for prompt 1 to demonstrate an improvement in the MAP score. Please document your code changes with comments and markdown cells so we can follow your thought process. (B) Please modify the code to make it more object oriented, more flexible to accommodate changes to the retrieval model, and getting it ready for production (such as adding logging, error handling, etc.)**\n",
    "\n",
    "For this case I selected (A) - I will give the option to use stemming or lemmatization to compare the final results. As you can see in the notebook, the results were in fact improved: Going from **0.38** to **0.42**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09ca7e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clone the git repo that contains the data and additional information about the dataset\n",
    "#!git clone https://github.com/wayfair/WANDS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b781e47-2986-4c1d-b1f6-28361312aa93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Plus\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721fdf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kaike\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK requirements\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eddff1",
   "metadata": {},
   "source": [
    "# Execution Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c182c2d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0786d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - query\n",
    "df_query = pd.read_csv(\"WANDS/dataset/query.csv\", sep='\\t')\n",
    "# Load dataset - products\n",
    "df_product = pd.read_csv(\"WANDS/dataset/product.csv\", sep='\\t')\n",
    "# Load dataset - labels\n",
    "df_label = pd.read_csv(\"WANDS/dataset/label.csv\", sep='\\t')\n",
    "df_label_grouped = df_label.groupby('query_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcafb78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Exact', 'Irrelevant', 'Partial']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify label options\n",
    "df_label['label'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80a395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to query to future evaluation - Strict Classes\n",
    "df_query['ids_true_partial'] = df_query['query_id'].apply(\n",
    "    lambda qid:\n",
    "    df_label.loc[(df_label['label'] == 'Partial') & (df_label['query_id'] == qid), 'product_id'].tolist()\n",
    ")\n",
    "df_query['ids_true_irrelevant'] = df_query['query_id'].apply(\n",
    "    lambda qid:\n",
    "    df_label.loc[(df_label['label'] == 'Irrelevant') & (df_label['query_id'] == qid), 'product_id'].tolist()\n",
    ")\n",
    "df_query['ids_true_exact'] = df_query['query_id'].apply(\n",
    "    lambda qid:\n",
    "    df_label.loc[(df_label['label'] == 'Exact') & (df_label['query_id'] == qid), 'product_id'].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6230074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN\n",
    "search_cols = ['product_name', 'product_class', 'category hierarchy', 'product_description']\n",
    "df_product[search_cols] = df_product[search_cols].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6e5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_product_search_column(name, class_type, category, description):\n",
    "    '''\n",
    "    Function to generate correctly the search column.\n",
    "    '''\n",
    "    # Construct\n",
    "    search_text = ' '.join([name, class_type, category, description])\n",
    "    search_text = re.sub(r'\\s+', ' ', search_text)\n",
    "    search_text = search_text.strip()\n",
    "    # Return\n",
    "    return search_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbff5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct search column over product\n",
    "df_product['search_text'] = df_product.apply(lambda row: \n",
    "                                             generate_product_search_column(\n",
    "                                                row['product_name'],\n",
    "                                                row['product_class'],\n",
    "                                                row['category hierarchy'],\n",
    "                                                row['product_description']\n",
    "                                            ),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f533679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_text_preprocessing(text, normalization='none'):\n",
    "    '''\n",
    "    Function to apply text pre-processing. This function will:\n",
    "    - Remove all punctuation, by adding a space in your place, except for numerical cases: 6.5 or 7,5 for example.\n",
    "    - Remove any sequence of blank space\n",
    "    - remove stopwords in english\n",
    "    - convert all to lower\n",
    "    - optionally apply lemmatization or stemming\n",
    "    '''\n",
    "    # Ensure we're working with string input\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "\n",
    "    # Initialize stemmer and lemmatizer if needed\n",
    "    stemmer = PorterStemmer() if normalization == 'stem' else None\n",
    "    lemmatizer = WordNetLemmatizer() if normalization == 'lemma' else None\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Handle special numerical cases (preserve decimals with . or ,)\n",
    "    # This pattern matches:\n",
    "    # 1. Standard numbers (123)\n",
    "    # 2. Numbers with decimal points (123.45 or .45)\n",
    "    # 3. Numbers with decimal commas (123,45 or ,45)\n",
    "    numeric_pattern = r'(?:\\d+[.,]\\d+|\\d+|[.,]\\d+)'\n",
    "    numeric_matches = re.findall(numeric_pattern, text)\n",
    "    \n",
    "    # First replace numeric patterns with temporary placeholders\n",
    "    placeholder = \" NUMERICPLACEHOLDERNOTHINGISEQUALTOTHIS \"\n",
    "    processed_text = re.sub(numeric_pattern, placeholder, text)\n",
    "    \n",
    "    # Remove all punctuation (replace with space)\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    processed_text = processed_text.translate(translator)\n",
    "    \n",
    "    # Tokenize and process words\n",
    "    words = processed_text.split(' ') # word_tokenize(processed_text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Apply normalization if requested\n",
    "    if normalization == 'stem' and stemmer:\n",
    "        normalized_words = [stemmer.stem(word) for word in filtered_words]\n",
    "    elif normalization == 'lemma' and lemmatizer:\n",
    "        normalized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    else:\n",
    "        normalized_words = filtered_words\n",
    "    \n",
    "    # Join words with single space and strip any leading/trailing whitespace\n",
    "    processed_text = ' '.join(normalized_words)\n",
    "    \n",
    "    # Remove any sequence of multiple spaces with single space\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()\n",
    "\n",
    "    # Returning ...\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea026cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing - products\n",
    "df_product['search_text_proc'] = df_product['search_text'].apply(lambda x: apply_text_preprocessing(x, 'none'))\n",
    "df_product['search_text_proc_stem'] = df_product['search_text'].apply(lambda x: apply_text_preprocessing(x, 'stem'))\n",
    "df_product['search_text_proc_lemma'] = df_product['search_text'].apply(lambda x: apply_text_preprocessing(x, 'lemma'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ad0f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply pre-processing - query\n",
    "df_query['query_proc'] = df_query['query'].apply(lambda x: apply_text_preprocessing(x))\n",
    "df_query['query_proc_stem'] = df_query['query'].apply(lambda x: apply_text_preprocessing(x, 'stem'))\n",
    "df_query['query_proc_lemma'] = df_query['query'].apply(lambda x: apply_text_preprocessing(x, 'lemma'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7c2144",
   "metadata": {},
   "source": [
    "## Recommendation System Engine\n",
    "\n",
    "Sparse Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfdf3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    '''\n",
    "    Simple tokenizer to avoid empty tokens.\n",
    "    '''\n",
    "    return [w for w in text.split(' ') if len(w) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ee94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_search_engine(corpus, indexes):\n",
    "    '''\n",
    "    Train BM-25.\n",
    "    '''\n",
    "    # Tokenize each search text\n",
    "    tokenized_corpus = [tokenize_text(product_text) for product_text in corpus]\n",
    "    # Generate the search engine\n",
    "    search_engine = BM25Plus(tokenized_corpus)\n",
    "    # Returning\n",
    "    return {'search_engine': search_engine, 'corpus': corpus, 'indexes': indexes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55e90fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_items(query_text, k, bm25_engine, bm25_indexes):\n",
    "    '''\n",
    "    Search the most relevant itens using bm-25.\n",
    "    '''\n",
    "    # Tokenize the text\n",
    "    tokenized_query = tokenize_text(query_text)\n",
    "    # Execute the search\n",
    "    top_searches_indexes = bm25_engine.get_top_n(tokenized_query, bm25_indexes, n=k)\n",
    "    # Returning\n",
    "    return top_searches_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb0c9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the search engine\n",
    "bm25 = generate_search_engine(df_product['search_text_proc'], df_product['product_id'])\n",
    "bm25_stem = generate_search_engine(df_product['search_text_proc_stem'], df_product['product_id'])\n",
    "bm25_lemma = generate_search_engine(df_product['search_text_proc_lemma'], df_product['product_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61df1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing one of the search system\n",
    "query_test = df_query.loc[17, 'query_proc']\n",
    "search_indexes_results = search_items(query_test, 10, bm25['search_engine'], bm25['indexes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e5677cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate for all query items\n",
    "df_query['ids_predicted'] = df_query.apply(lambda x: search_items(x['query_proc'], 10, bm25['search_engine'], bm25['indexes']), axis=1)\n",
    "df_query['ids_predicted_stem'] = df_query.apply(lambda x: search_items(x['query_proc_stem'], 10, bm25_stem['search_engine'], bm25_stem['indexes']), axis=1)\n",
    "df_query['ids_predicted_lemma'] = df_query.apply(lambda x: search_items(x['query_proc_lemma'], 10, bm25_lemma['search_engine'], bm25_lemma['indexes']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f21eb2",
   "metadata": {},
   "source": [
    "## System Evaluation\n",
    "\n",
    "Strict Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d5eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k(true_ids, predicted_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision at K (MAP@K).\n",
    "\n",
    "    Parameters:\n",
    "        true_ids (list): List of relevant product IDs.\n",
    "        predicted_ids (list): List of predicted product IDs.\n",
    "        k (int): Number of top elements to consider.\n",
    "            NOTE: IF you wish to change top k, please provide a justification for choosing the new value\n",
    "\n",
    "    Returns:\n",
    "        float: MAP@K score.\n",
    "    \"\"\"\n",
    "    #if either list is empty, return 0\n",
    "    if not len(true_ids) or not len(predicted_ids):\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p_id in enumerate(predicted_ids[:k]):\n",
    "        if p_id in true_ids and p_id not in predicted_ids[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / min(len(true_ids), k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77a154da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mean Precision@10 EXACT:  0.38\n",
      "> Mean Precision@10 PARTIAL:  0.32\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@10\n",
    "df_query['precision_at_10_exact'] = df_query.apply(lambda x: map_at_k(x['ids_true_exact'], x['ids_predicted']), axis=1)\n",
    "df_query['precision_at_10_partial'] = df_query.apply(lambda x: map_at_k(x['ids_true_partial'], x['ids_predicted']), axis=1)\n",
    "\n",
    "# Check Mean over the test dataset\n",
    "print('> Mean Precision@10 EXACT: ', df_query['precision_at_10_exact'].mean().round(2))\n",
    "print('> Mean Precision@10 PARTIAL: ', df_query['precision_at_10_partial'].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c86d3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mean Precision@10 EXACT:  0.38\n",
      "> Mean Precision@10 EXACT - STEM:  0.42\n",
      "> Mean Precision@10 EXACT - LEMMA:  0.42\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@10\n",
    "df_query['precision_at_10_exact'] = df_query.apply(lambda x: map_at_k(x['ids_true_exact'], x['ids_predicted']), axis=1)\n",
    "df_query['precision_at_10_exact_stem'] = df_query.apply(lambda x: map_at_k(x['ids_true_exact'], x['ids_predicted_stem']), axis=1)\n",
    "df_query['precision_at_10_exact_lemma'] = df_query.apply(lambda x: map_at_k(x['ids_true_exact'], x['ids_predicted_lemma']), axis=1)\n",
    "\n",
    "# Check Mean over the test dataset\n",
    "print('> Mean Precision@10 EXACT: ', df_query['precision_at_10_exact'].mean().round(2))\n",
    "print('> Mean Precision@10 EXACT - STEM: ', df_query['precision_at_10_exact_stem'].mean().round(2))\n",
    "print('> Mean Precision@10 EXACT - LEMMA: ', df_query['precision_at_10_exact_lemma'].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6a5847",
   "metadata": {},
   "source": [
    "# System Evaluation\n",
    "\n",
    "Partial Included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f627972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights\n",
    "EXACT_WEIGHT = 1.0\n",
    "PARTIAL_WEIGHT = 0.5\n",
    "\n",
    "# Construct IDs / Weight\n",
    "df_query['ids_true_exact_partial'] = df_query['query_id'].apply(\n",
    "    lambda qid:\n",
    "    df_label.loc[(df_label['label'].isin(['Exact', 'Partial'])) & (df_label['query_id'] == qid), 'product_id'].tolist()\n",
    ")\n",
    "df_query['weight_ids_true_exact_partial'] = df_query['query_id'].apply(\n",
    "    lambda qid:\n",
    "    df_label.loc[(df_label['label'].isin(['Exact', 'Partial'])) & (df_label['query_id'] == qid), 'label'].tolist()\n",
    ")\n",
    "df_query['weight_ids_true_exact_partial'] = df_query['weight_ids_true_exact_partial'].apply(\n",
    "    lambda x_list:\n",
    "    [EXACT_WEIGHT if x == 'Exact' else PARTIAL_WEIGHT for x in x_list]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad66a93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_at_k_weighted(true_ids, class_weights, predicted_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the Custom Mean Average Precision Weighted at K (MAP@K).\n",
    "    \"\"\"\n",
    "    # if either list is empty, return 0\n",
    "    if not len(true_ids) or not len(predicted_ids):\n",
    "        return 0.0\n",
    "\n",
    "    # Create a dictionary to map true_ids to their corresponding weights\n",
    "    true_id_to_weight = {true_id: weight for true_id, weight in zip(true_ids, class_weights)}\n",
    "\n",
    "    # Initiate the variables\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    sum_weights = 0.0\n",
    "\n",
    "    # Loop over the predicted results\n",
    "    for i, p_id in enumerate(predicted_ids[:k]):\n",
    "        # Checking\n",
    "        if p_id in true_ids and p_id not in predicted_ids[:i]:\n",
    "            # Get the weight of the current true_id\n",
    "            weight = true_id_to_weight[p_id]\n",
    "            num_hits += weight\n",
    "            score += (num_hits / (i + 1.0)) * weight\n",
    "            sum_weights += weight\n",
    "\n",
    "    # Normalize by the sum of weights of the relevant items in top-k or all true_ids, whichever is smaller\n",
    "    if sum_weights > 0:\n",
    "        return score / min(len(true_ids), k)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4343d679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mean Precision@10 WEIGHTED:  0.42\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@10 - Weighted Matches\n",
    "df_query['precision_at_10_weighted'] = df_query.apply(\n",
    "    lambda x: \n",
    "    map_at_k_weighted(x['ids_true_exact_partial'], x['weight_ids_true_exact_partial'], x['ids_predicted']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check Mean over the test dataset\n",
    "print('> Mean Precision@10 WEIGHTED: ', df_query['precision_at_10_weighted'].mean().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8258900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Mean Precision@10 EXACT VALIDATION:  0.38\n"
     ]
    }
   ],
   "source": [
    "# Calculate Precision@10 - Exact Matches (Weighted Version) - Metric validation\n",
    "df_query['precision_at_10_exact_validation'] = df_query.apply(\n",
    "    lambda x: \n",
    "    map_at_k_weighted(x['ids_true_exact'], [1 for i in range(0, len(x['ids_true_exact']))], x['ids_predicted']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Check Mean over the test dataset\n",
    "print('> Mean Precision@10 EXACT VALIDATION: ', df_query['precision_at_10_exact_validation'].mean().round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_hbs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
